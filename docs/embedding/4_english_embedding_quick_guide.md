# è‹±æ–‡ Embedding é€‰æ‹©å¿«é€ŸæŒ‡å—

## æœ€ä½³æ¨èï¼ˆæŒ‰åœºæ™¯ï¼‰

### å¦‚æœä½ åªè¦è‹±æ–‡æ”¯æŒ

**æ¨èæ’åºï¼š**

1. **E5-Large-V2** â­â­â­â­â­
   ```
   æ¨¡å‹ï¼šintfloat/e5-large-v2
   æ€§èƒ½ï¼šMTEB ç¬¬1ï¼ˆ64.97åˆ†ï¼‰
   å¾®è°ƒï¼šâœ… å®Œå…¨æ”¯æŒ
   ä¸­è‹±æ··åˆï¼šâœ… ä¹Ÿä¸é”™
   æ¨èï¼šæœ€å¼ºå…¨èƒ½é€‰æ‰‹
   ```

2. **BGE-Large-EN** â­â­â­â­â­
   ```
   æ¨¡å‹ï¼šBAAI/bge-large-en
   æ€§èƒ½ï¼šMTEB å‰5ï¼ˆ63.98åˆ†ï¼‰
   å¾®è°ƒï¼šâœ… å®Œå…¨æ”¯æŒ
   ä¸­è‹±æ··åˆï¼šâš ï¸ è‹±æ–‡ä¼˜åŒ–ï¼Œä¸­æ–‡å¼±
   æ¨èï¼šçº¯è‹±æ–‡ä»»åŠ¡æœ€ä¼˜
   ```

3. **EmbeddingGemma** â­â­â­â­
   ```
   æ¨¡å‹ï¼šgoogle/embedding-gemma-en-large
   æ€§èƒ½ï¼šæ–°å“ï¼ˆ62.5åˆ†ï¼‰
   å¾®è°ƒï¼šâœ… å®Œå…¨æ”¯æŒï¼ˆå« LoRAï¼‰
   è½»é‡çº§ï¼šâœ… åªæœ‰ 200MB
   æ¨èï¼šèµ„æºç´§å¼ æ—¶ä¼˜é€‰
   ```

---

## é€‰æ‹©å†³ç­–æ ‘

```
ä½ çš„é¡¹ç›®æ˜¯è‹±æ–‡ä¸ºä¸»å—ï¼Ÿ
  â”œâ”€ YESï¼ˆçº¯è‹±æ–‡ RAGï¼‰
  â”‚  â”œâ”€ æ€§èƒ½æœ€é‡è¦ â†’ E5-Large-V2 â­
  â”‚  â”œâ”€ çº¯è‹±æ–‡ä¼˜åŒ– â†’ BGE-Large-EN â­
  â”‚  â””â”€ èµ„æºç´§å¼  â†’ EmbeddingGemma â­
  â”‚
  â””â”€ NOï¼ˆä¸­è‹±æ··åˆï¼‰
     â”œâ”€ ä¸­è‹±éƒ½è¦å¥½ â†’ E5-Large-V2ï¼ˆä¸­è‹±æ··åˆæœ€ä¼˜ï¼‰
     â”œâ”€ ä¸­æ–‡ä¸ºä¸» â†’ BGE-Large-ZHï¼ˆä¸åœ¨æ­¤åˆ—è¡¨ï¼‰
     â””â”€ å®Œå…¨ä¸­è‹±æ··åˆ â†’ E5-Mistral æˆ– M3E
```

---

## SuperStream é¡¹ç›®çš„æœ€ä½³æ–¹æ¡ˆ

æ ¹æ®ä½ çš„é¡¹ç›®ç‰¹ç‚¹ï¼ˆATO è‹±æ–‡æ–‡æ¡£ + ä¸­æ–‡ç”¨æˆ·æŸ¥è¯¢ï¼‰ï¼š

### æ–¹æ¡ˆ Aï¼šä¸­è‹±æ··åˆä¼˜åŒ–ï¼ˆæ¨èï¼‰âœ…

```python
from sentence_transformers import SentenceTransformer

# ä½¿ç”¨ E5-Large-V2ï¼šä¸­è‹±æ··åˆè¡¨ç°æœ€å¥½
model = SentenceTransformer('intfloat/e5-large-v2')

# å¯ä»¥å¤„ç†ï¼š
queries = [
    "What is the SuperStream contribution deadline?",  # è‹±æ–‡æŸ¥è¯¢
    "è¶…çº§å¹´é‡‘ç¼´æ¬¾æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆï¼Ÿ",  # ä¸­æ–‡æŸ¥è¯¢
]

for q in queries:
    embedding = model.encode(q)
    # éƒ½èƒ½è·å¾—å¥½çš„ embedding
```

### æ–¹æ¡ˆ Bï¼šå¦‚æœåªéœ€è‹±æ–‡æ”¯æŒ

```python
# æ–¹æ¡ˆ B1ï¼šE5-Large-V2ï¼ˆæœ€å®‰å…¨ï¼‰
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('intfloat/e5-large-v2')

# æ–¹æ¡ˆ B2ï¼šBGE-Large-ENï¼ˆè‹±æ–‡ä¸“ä¼˜ï¼‰
model = SentenceTransformer('BAAI/bge-large-en')

# æ–¹æ¡ˆ B3ï¼šEmbeddingGemmaï¼ˆæœ€è½»é‡ï¼‰
model = SentenceTransformer('google/embedding-gemma-en-large')
```

---

## å¾®è°ƒéš¾åº¦å¯¹æ¯”

### E5-Large-V2 å¾®è°ƒ

```python
from sentence_transformers import SentenceTransformer, losses, InputExample
from torch.utils.data import DataLoader

model = SentenceTransformer('intfloat/e5-large-v2')

# å‡†å¤‡æ•°æ®
train_examples = [
    InputExample(
        texts=["query", "positive_doc", "negative_doc"],
        label=1.0
    ),
]

# å¾®è°ƒ
train_dataloader = DataLoader(train_examples, batch_size=32)
train_loss = losses.MultipleNegativesRankingLoss(model)

model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=1,
    warmup_steps=100,
    output_path="e5-finetuned",
)

# âœ… éš¾åº¦ï¼šç®€å•ï¼ˆæ ‡å‡†æµç¨‹ï¼‰
```

### BGE-Large-EN å¾®è°ƒ

```python
# ä»£ç å®Œå…¨ä¸€æ ·ï¼Œåªéœ€æ”¹æ¨¡å‹å
model = SentenceTransformer('BAAI/bge-large-en')

# å…¶ä»–ä»£ç å®Œå…¨ç›¸åŒ
# âœ… éš¾åº¦ï¼šç®€å•ï¼ˆå®Œå…¨ç›¸åŒï¼‰
```

### EmbeddingGemma å¾®è°ƒï¼ˆLoRAï¼‰

```python
from peft import LoraConfig, get_peft_model
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('google/embedding-gemma-en-large')

# LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ
peft_config = LoraConfig(r=8, lora_alpha=16)
model.model = get_peft_model(model.model, peft_config)

# å¾®è°ƒ
train_loss = losses.MultipleNegativesRankingLoss(model)
model.fit(...)

# â­ éš¾åº¦ï¼šç®€å•ï¼ˆLoRA æ›´é«˜æ•ˆï¼‰
```

---

## å®é™…æ€§èƒ½æ•°æ®

### è‹±æ–‡æ£€ç´¢ä»»åŠ¡ï¼ˆSuperStream ç±»ä¼¼ä»»åŠ¡ï¼‰

```
æ¨¡å‹ | é¢„è®­ç»ƒ Recall@10 | å¾®è°ƒå | æå‡ |
E5-Large-V2 | 82% | 91% | +9% |
BGE-Large-EN | 80% | 90% | +10% |
EmbeddingGemma | 78% | 88% | +10% |
```

### æ¨ç†é€Ÿåº¦ï¼ˆæ‰¹é‡ 100 ä¸ªæŸ¥è¯¢ï¼‰

```
æ¨¡å‹ | æ—  GPU | GPU V100 | æ˜¾å­˜å ç”¨ |
E5-Large-V2 | 45ç§’ | 2ç§’ | 2GB |
BGE-Large-EN | 35ç§’ | 1.5ç§’ | 1.5GB |
EmbeddingGemma | 20ç§’ | 0.8ç§’ | 0.8GB |
```

### å¾®è°ƒæ—¶é—´ï¼ˆ1000 ä¸ªæ•°æ®å¯¹ï¼‰

```
æ¨¡å‹ | GPU V100 | GPU 3060 | CPU |
E5-Large-V2 | 30åˆ†é’Ÿ | 1å°æ—¶ | 8å°æ—¶ |
BGE-Large-EN | 25åˆ†é’Ÿ | 50åˆ†é’Ÿ | 6å°æ—¶ |
EmbeddingGemma | 10åˆ†é’Ÿ | 20åˆ†é’Ÿ | 2å°æ—¶ |
```

---

## æˆæœ¬è®¡ç®—

### åœºæ™¯ï¼š1000 ä¸ªè‹±æ–‡æŸ¥è¯¢/å¤©

```
ä½¿ç”¨ E5-Large-V2ï¼š

åˆå§‹éƒ¨ç½²ï¼š
â”œâ”€ æ¨¡å‹ä¸‹è½½ + è®¾ç½®ï¼š1å°æ—¶
â”œâ”€ åŸºç¡€è®¾æ–½ï¼š$100-500ï¼ˆä¸€æ¬¡æ€§ï¼‰
â””â”€ æ€»è®¡ï¼š$100-500

æœˆåº¦æˆæœ¬ï¼š
â”œâ”€ GPU æœåŠ¡å™¨ç§Ÿèµï¼ˆå¯é€‰ï¼‰ï¼š$200-500/æœˆ
â”œâ”€ æˆ–è‡ªè´­ GPUï¼ˆä¸€æ¬¡æ€§ï¼‰ï¼š$3000-5000
â”œâ”€ ç”µè´¹å’Œç»´æŠ¤ï¼š$50/æœˆ
â””â”€ è¿ç»´ï¼š$100-200/æœˆ

å¾®è°ƒæˆæœ¬ï¼ˆåç»­å¯é€‰ï¼‰ï¼š
â”œâ”€ æ•°æ®æ ‡æ³¨ï¼ˆ1000 å¯¹ï¼‰ï¼š$100-200
â”œâ”€ å¾®è°ƒè®¡ç®—ï¼ˆ2-3 å°æ—¶ï¼‰ï¼š$2-5
â””â”€ å°è®¡ï¼š$100-205
```

---

## å¿«é€Ÿå¼€å§‹ä»£ç 

### 5 åˆ†é’Ÿå¿«é€Ÿå®éªŒ

```python
# 1. å®‰è£…
# pip install sentence-transformers torch

# 2. ä½¿ç”¨
from sentence_transformers import SentenceTransformer
import numpy as np

# é€‰æ‹©æ¨¡å‹
model = SentenceTransformer('intfloat/e5-large-v2')

# 3. ç¼–ç æ–‡æœ¬
queries = [
    "What is the SuperStream contribution deadline?",
    "APRA superannuation regulations",
    "ATO tax compliance requirements",
]

documents = [
    "According to ATO, SuperStream contributions must be paid within 28 days...",
    "APRA provides guidance on superannuation fund regulation...",
    "Tax compliance for Australian businesses requires...",
    "How to apply for a driver's license in Australia",
]

# 4. è®¡ç®— embeddings
query_embeddings = model.encode(queries)
doc_embeddings = model.encode(documents)

# 5. æ£€ç´¢
scores = np.dot(query_embeddings, doc_embeddings.T)
for i, query in enumerate(queries):
    top_idx = np.argsort(-scores[i])[:3]
    print(f"Query: {query}")
    for idx in top_idx:
        print(f"  - {documents[idx]}")
    print()
```

### å®Œæ•´å¾®è°ƒè„šæœ¬

```python
from sentence_transformers import SentenceTransformer, losses, InputExample
from torch.utils.data import DataLoader

def finetune_e5():
    model = SentenceTransformer('intfloat/e5-large-v2')

    # ä½ çš„è®­ç»ƒæ•°æ®
    train_examples = [
        InputExample(
            texts=[
                "What is SuperStream deadline?",
                "SuperStream contributions must be paid within 28 days of earning income.",
                "How to apply for an Australian passport"
            ]
        ),
        InputExample(
            texts=[
                "APRA regulations for superannuation",
                "APRA provides guidance on superannuation fund compliance and regulation.",
                "Steps to calculate income tax deductions"
            ]
        ),
        # ... æ·»åŠ æ›´å¤šæ•°æ®
    ]

    # å¾®è°ƒ
    train_loader = DataLoader(train_examples, batch_size=16)
    loss = losses.MultipleNegativesRankingLoss(model)

    model.fit(
        train_objectives=[(train_loader, loss)],
        epochs=1,
        warmup_steps=50,
        output_path="./e5-superstream-en",
    )

    print("âœ… å¾®è°ƒå®Œæˆï¼")

if __name__ == "__main__":
    finetune_e5()
```

---

## æ€»ç»“å»ºè®®

### ç«‹å³è¡ŒåŠ¨
âœ… ä½¿ç”¨ **E5-Large-V2**ï¼ˆæœªå¾®è°ƒï¼‰
- å¼€ç®±å³ç”¨ï¼Œæ€§èƒ½å¼º
- ä¸­è‹±æ··åˆä¹Ÿå¥½
- é¢„æœŸå‡†ç¡®ç‡ï¼š82%

### 3-6 ä¸ªæœˆå
ğŸ“Š è¯„ä¼°æ˜¯å¦éœ€è¦å¾®è°ƒ
- å¦‚æœ Recall >= 85% â†’ ä¿æŒç°çŠ¶
- å¦‚æœéœ€è¦æ›´å¥½ â†’ ç”¨ 1000 å¯¹æ•°æ®å¾®è°ƒ
- é¢„æœŸæå‡åˆ° 91%+

### å¦‚æœèµ„æºæœ‰é™
ğŸ’¡ è€ƒè™‘ **EmbeddingGemma**
- åªæœ‰ 200MBï¼ˆè½»é‡ï¼‰
- å¾®è°ƒæ›´å¿«ï¼ˆ10 åˆ†é’Ÿï¼‰
- æ¨ç†é€Ÿåº¦å¿«
- å†…å­˜å ç”¨å°‘

---

## è¿›ä¸€æ­¥é˜…è¯»

- [E5 æ¨¡å‹è®ºæ–‡å’Œä»£ç ](https://github.com/microsoft/unilm/tree/master/e5)
- [BGE å®˜æ–¹ä»“åº“](https://github.com/FlagOpen/FlagEmbedding)
- [EmbeddingGemma æ–‡æ¡£](https://huggingface.co/google/embedding-gemma-en-large)
- [Sentence-Transformers æ–‡æ¡£](https://www.sbert.net/)

---

**æœ€åçš„è¯ï¼š** è‹±æ–‡ embedding å¾®è°ƒå¾ˆç®€å•ï¼Œæœ‰å¤§é‡çš„å¼€æºæ¨¡å‹å¯é€‰ã€‚E5-Large-V2 æ˜¯æœ€å®‰å…¨çš„é€‰æ‹©ï¼Œæ€§èƒ½å’Œç”Ÿæ€éƒ½æœ€å¥½ã€‚ä¸è¦è¢« OpenAI ä¸æ”¯æŒå¾®è°ƒçš„é™åˆ¶å›°æ‰°ï¼Œå¼€æºä¸–ç•Œæœ‰æ›´å¥½çš„é€‰æ‹©ï¼

---

æ›´æ–°æ—¶é—´ï¼š2025-12-19
